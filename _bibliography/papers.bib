---
---

@misc{charton2024patternboostconstructionsmathematicslittle,
      title={PatternBoost: Constructions in Mathematics with a Little Help from AI}, 
      author={François Charton and Jordan S. Ellenberg and Adam Zsolt Wagner and Geordie Williamson},
      year={2024},
      eprint={2411.00566},
      archivePrefix={arXiv},
      primaryClass={math.CO},
	  abstract={We introduce PatternBoost, a flexible method for finding interesting constructions in mathematics. Our algorithm alternates between two phases. In the first ``local'' phase, a classical search algorithm is used to produce many desirable constructions. In the second ``global'' phase, a transformer neural network is trained on the best such constructions. Samples from the trained transformer are then used as seeds for the first phase, and the process is repeated. We give a detailed introduction to this technique, and discuss the results of its application to several problems in extremal combinatorics. The performance of PatternBoost varies across different problems, but there are many situations where its performance is quite impressive. Using our technique, we find the best known solutions to several long-standing problems, including the construction of a counterexample to a conjecture that had remained open for 30 years. },
      html={https://arxiv.org/abs/2411.00566}, 
	  preview={williamson.png}
}

@misc{yang2024formalmathematicalreasoningnew,
      title={Formal Mathematical Reasoning: A New Frontier in AI}, 
      author={Kaiyu Yang and Gabriel Poesia and Jingxuan He and Wenda Li and Kristin Lauter and Swarat Chaudhuri and Dawn Song},
      year={2024},
      eprint={2412.16075},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
	  abstract={AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.},
      html={https://arxiv.org/abs/2412.16075}, 
	  preview={lauter.png}
}

@book {MR4235100,
    AUTHOR = {Fefferman, Charles and Israel, Arie},
     TITLE = {Fitting smooth functions to data},
    SERIES = {CBMS Regional Conference Series in Mathematics},
    VOLUME = {135},
 PUBLISHER = {American Mathematical Society, Providence, RI},
      YEAR = {2020},
     PAGES = {x+160},
      ISBN = {978-1-4704-6130-0},
html={https://bookstore.ams.org/view?ProductCode=CBMS/135},
	   preview = {fefferman.png}
}


@article {MR1308702,
    AUTHOR = {Fefferman, Charles},
     TITLE = {Reconstructing a neural net from its output},
   JOURNAL = {Rev. Mat. Iberoamericana},
  FJOURNAL = {Revista Matem\'atica Iberoamericana},
    VOLUME = {10},
      YEAR = {1994},
    NUMBER = {3},
     PAGES = {507--555},
      ISSN = {0213-2230},
       DOI = {10.4171/RMI/160},
       html = {https://doi.org/10.4171/RMI/160},
	   preview = {fefferman.png}
}


@article{LEI2020361,
title = {A Geometric Understanding of Deep Learning},
journal = {Engineering},
volume = {6},
number = {3},
pages = {361-374},
year = {2020},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2095809919302279},
author = {Na Lei and Dongsheng An and Yang Guo and Kehua Su and Shixia Liu and Zhongxuan Luo and Shing-Tung Yau and Xianfeng Gu},
keywords = {Generative, Adversarial, Deep learning, Optimal transportation, Mode collapse},
abstract = {This work introduces an optimal transportation (OT) view of generative adversarial networks (GANs). Natural datasets have intrinsic patterns, which can be summarized as the manifold distribution principle: the distribution of a class of data is close to a low-dimensional manifold. GANs mainly accomplish two tasks: manifold learning and probability distribution transformation. The latter can be carried out using the classical OT method. From the OT perspective, the generator computes the OT map, while the discriminator computes the Wasserstein distance between the generated data distribution and the real data distribution; both can be reduced to a convex geometric optimization process. Furthermore, OT theory discovers the intrinsic collaborative—instead of competitive—relation between the generator and the discriminator, and the fundamental reason for mode collapse. We also propose a novel generative model, which uses an autoencoder (AE) for manifold learning and OT map for probability distribution transformation. This AE–OT model improves the theoretical rigor and transparency, as well as the computational stability and efficiency; in particular, it eliminates the mode collapse. The experimental results validate our hypothesis, and demonstrate the advantages of our proposed model.},
html = {https://doi.org/10.1016/j.eng.2019.09.010},
preview={yau2.png}
}


@inbook{Berner_2022,
   title={The Modern Mathematics of Deep Learning},
   ISBN={9781316516782},
   DOI={10.1017/9781009025096.002},
   booktitle={Mathematical Aspects of Deep Learning},
   publisher={Cambridge University Press},
   author={Berner, Julius and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
   year={2022},
   month=dec,
   pages={1–111},
   abstract={We describe the new field of mathematical analysis of deep learning. This field emerged around a list of research questions that were not answered within the classical framework of learning theory. These questions concern: the outstanding generalization power of overparametrized neural networks, the role of depth in deep architectures, the apparent absence of the curse of dimensionality, the surprisingly successful optimization performance despite the non-convexity of the problem, understanding what features are learned, why deep architectures perform exceptionally well in physical problems, and which fine aspects of an architecture affect the behavior of a learning task in which way. We present an overview of modern approaches that yield partial answers to these questions. For selected approaches, we describe the main ideas in more detail. },
   html={http://dx.doi.org/10.1017/9781009025096.002},
   preview={kutyniok.png}
   }

@misc{belfiore2022topos,
      title={Topos and Stacks of Deep Neural Networks}, 
      author={Jean-Claude Belfiore and Daniel Bennequin},
      year={2022},
      eprint={2106.14587},
      archivePrefix={arXiv},
      primaryClass={math.AT},
	  abstract={Every known artificial deep neural network (DNN) corresponds to an object in a canonical Grothendieck's topos; its learning dynamic corresponds to a flow of morphisms in this topos. Invariance structures in the layers (like CNNs or LSTMs) correspond to Giraud's stacks. This invariance is supposed to be responsible of the generalization property, that is extrapolation from learning data under constraints. The fibers represent pre-semantic categories (Culioli, Thom), over which artificial languages are defined, with internal logics, intuitionist, classical or linear (Girard). Semantic functioning of a network is its ability to express theories in such a language for answering questions in output about input data. Quantities and spaces of semantic information are defined by analogy with the homological interpretation of Shannon's entropy of P.Baudot and D.Bennequin in 2015). They generalize the measures found by Carnap and Bar-Hillel (1952). Amazingly, the above semantical structures are classified by geometric fibrant objects in a closed model category of Quillen, then they give rise to homotopical invariants of DNNs and of their semantic functioning. Intentional type theories (Martin-Loef) organize these objects and fibrations between them. Information contents and exchanges are analyzed by Grothendieck's derivators.},
	  html={https://arxiv.org/abs/2106.14587},
	  preview={belfiore.png}
}

@misc{joharinad2022geometry,
      author={Parvaneh Joharinad and Jürgen Jost},
      title={Geometry of Data}, 
      journal={arXiv preprint 2203.07208},
	  volume={2203.07208},
	  pages={16 p.},
      primaryClass={math.MG},
	  abstract={Topological data analysis asks when balls in a metric space (X,d) intersect. Geometric data analysis asks how much balls have to be enlarged to intersect. We connect this principle to the traditional core geometric concept of curvature. This enables us, on one hand, to reconceptualize curvature and link it to the geometric notion of hyperconvexity. On the other hand, we can then also understand methods of topological data analysis from a geometric perspective. },
	  year={2022},
	  html={https://arxiv.org/abs/2203.07208},
	  preview={jost.png}
}


@article{https://doi.org/10.1002/dvdy.175,
author = {Amézquita, Erik J. and Quigley, Michelle Y. and Ophelders, Tim and Munch, Elizabeth and Chitwood, Daniel H.},
title = {The shape of things to come: Topological data analysis and biology, from molecules to organisms},
journal = {Developmental Dynamics},
volume = {249},
number = {7},
pages = {816-833},
keywords = {biology, data science, mathematical biology, persistent homology, shape, topological data analysis},
url = {https://anatomypubs.onlinelibrary.wiley.com/doi/abs/10.1002/dvdy.175},
eprint = {https://anatomypubs.onlinelibrary.wiley.com/doi/pdf/10.1002/dvdy.175},
abstract = {Abstract Shape is data and data is shape. Biologists are accustomed to thinking about how the shape of biomolecules, cells, tissues, and organisms arise from the effects of genetics, development, and the environment. Less often do we consider that data itself has shape and structure, or that it is possible to measure the shape of data and analyze it. Here, we review applications of topological data analysis (TDA) to biology in a way accessible to biologists and applied mathematicians alike. TDA uses principles from algebraic topology to comprehensively measure shape in data sets. Using a function that relates the similarity of data points to each other, we can monitor the evolution of topological features—connected components, loops, and voids. This evolution, a topological signature, concisely summarizes large, complex data sets. We first provide a TDA primer for biologists before exploring the use of TDA across biological sub-disciplines, spanning structural biology, molecular biology, evolution, and development. We end by comparing and contrasting different TDA approaches and the potential for their use in biology. The vision of TDA, that data are shape and shape is data, will be relevant as biology transitions into a data-driven era where the meaningful interpretation of large data sets is a limiting factor.},
year = {2020},
html = {https://doi.org/10.1002/dvdy.175},
preview = {munchbio.png}
}


@article{Munch_2017, title={A User’s Guide to Topological Data Analysis}, volume={4}, url={https://learning-analytics.info/index.php/JLA/article/view/5196}, DOI={10.18608/jla.2017.42.6}, abstract={Topological data analysis (TDA) is a collection of powerful tools that can quantify shape and structure in data in order to answer questions from the data’s domain. This is done by representing some aspect of the structure of the data in a simplified topological signature. In this article, we introduce two of the most commonly used topological signatures. First, the persistence diagram represents loops and holes in the space by considering connectivity of the data points for a continuum of values rather than a single fixed value. The second topological signature, the mapper graph, returns a 1-dimensional structure representing the shape of the data, and is particularly good for exploration and visualization of the data. While these techniques are based on very sophisticated mathematics, the current ubiquity of available software means that these tools are more accessible than ever to be applied to data by researchers in education and learning, as well as all domain scientists.}, number={2}, journal={Journal of Learning Analytics}, author={Munch, Elizabeth}, year={2017}, month={Jul.}, pages={47–61}, html={https://doi.org/10.18608/jla.2017.42.6}, preview={tda.png} }


@article{davies2021advancing,
  title={Advancing mathematics by guiding human intuition with AI},
  author={Davies, Alex and Veli{\v{c}}kovi{\'c}, Petar and Buesing, Lars and Blackwell, Sam and Zheng, Daniel and Toma{\v{s}}ev, Nenad and Tanburn, Richard and Battaglia, Peter and Blundell, Charles and Juh{\'a}sz, Andr{\'a}s and Lackenby, Marc and  Williamson, Geordie and  Hassabis, Demis and   Kohli, Pushmeet},
  journal={Nature},
  volume={600},
  number={7887},
  pages={70--74},
  year={2021},
  publisher={Nature Publishing Group},
  abstract={The practice of mathematics involves discovering patterns and using these to formulate and prove conjectures, resulting in theorems. Since the 1960s, mathematicians have used computers to assist in the discovery of patterns and formulation of conjectures1, most famously in the Birch and Swinnerton-Dyer conjecture2, a Millennium Prize Problem3. Here we provide examples of new fundamental results in pure mathematics that have been discovered with the assistance of machine learning—demonstrating a method by which machine learning can aid mathematicians in discovering new conjectures and theorems. We propose a process of using machine learning to discover potential patterns and relations between mathematical objects, understanding them with attribution techniques and using these observations to guide intuition and propose conjectures. We outline this machine-learning-guided framework and demonstrate its successful application to current research questions in distinct areas of pure mathematics, in each case showing how it led to meaningful mathematical contributions on important open problems: a new connection between the algebraic and geometric structure of knots, and a candidate algorithm predicted by the combinatorial invariance conjecture for symmetric groups. Our work may serve as a model for collaboration between the fields of mathematics and artificial intelligence (AI) that can achieve surprising results by leveraging the respective strengths of mathematicians and machine learning.},
  html={https://www.nature.com/articles/s41586-021-04086-x},
  preview={nature.png}
}
